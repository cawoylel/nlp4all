{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jKa71Y6_VYK"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx2GbQYWA6L8"
      },
      "source": [
        "Welcome to our latest adventure at Cawoylel! If youâ€™ve been following us, you know we hold a visionary perspective that embraces the powerful role of technology in **preserving and promoting linguistic and cultural diversity**. **Our journey began with Fula**, where we developed **Windanam**, the first Fula multidialectal speech recognition model. But our mission extends far beyond a single language. We believe in open-source, collaborative approaches that can democratize AI for all of Africa. The recent success with Windanam sparked a question: *can other African languages follow suit?*\n",
        "\n",
        "We're a glad to introduce the **NLP4ALL** ASR tutorial, a detailed guide developed by our **NLP Lead** and **Co-Founder - Yaya Sy**, to empower communities to create speech recognition solutions for their languages! This step-by-step guide equips you with the knowledge to develop your own ASR solution. By using **Seerer**, a language spoken in Senegal, as a case study, we aim to provide a replicable approach that can be adapted to any African language. Our goal is to make this technology accessible, offering a template that communities can use to ensure their language thrive in the digital age.\n",
        "\n",
        "In this tutorial, we break down the entire process, from data collection (using the Bible as an example) to model creation. We'll cover *audio segmentation*, *transcription alignment*, *data preparation* with Hugging Face ðŸ¤—, and finally, building your ASR model by *fine-tuning an existing open-source model*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkBwANXMA6MB"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. [Setup and Configuration](#setup-and-configuration)\n",
        "2. [Getting the ASR Data](#data-collection)\n",
        "    - 2.1 [Bible Crawler](#bible-crawler)\n",
        "    - 2.2 [Resampling the Audios](#resampling-the-audios)\n",
        "    - 2.3 [Neural Forced Alignment](#neural-forced-alignment)\n",
        "3. [Data Preprocessing](#data-preprocessing)\n",
        "    - 3.1 [Preparing the HuggingFace Dataset](#preparing-the-huggingface-dataset)\n",
        "    - 3.2 [Preprocessing the Dataset](#preprocessing-the-dataset)\n",
        "4. [Fine-Tuning Whisper Large on Seereer ASR Dataset](#finetuning)\n",
        "    - 4.1 [Load Model](#load-model)\n",
        "    - 4.2 [Prepare the Pretrained Model for LoRA](#prepare-the-pretrained-model-for-lora)\n",
        "    - 4.3 [Attach Adapters to the Pretrained Model](#attach-adapters-to-the-pretrained-model)\n",
        "    - 4.4 [Data Loader](#data-loader)\n",
        "    - 4.5 [Training Arguments](#training-arguments)\n",
        "    - 4.6 [Launch Training and Evaluate](#launch-training-and-evaluate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZM5ZQKY_SEJ"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYPVBNTUA6MD"
      },
      "source": [
        "To kickstart the setup process, we need to ensure that our environment has all the necessary dependencies installed. First, we update the package list with `!apt-get update`. Then, we install essential packages required for audio processing, such as `libsox-fmt-all`, `sox`, and `ffmpeg`. Additionally, we install `libicu-dev` and `pkg-config` to handle text processing and Unicode symbols effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddzlqgyTNrVB"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install libsox-fmt-all sox ffmpeg # needed for processing audio\n",
        "!apt install libicu-dev pkg-config # needed for processing text and unicode symbols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIR7MAujA6MG"
      },
      "source": [
        "Next, we focus on installing the PyTorch nightly version, which is crucial for compatibility with other libraries and tools. We begin by uninstalling existing versions of `torch`, `torchaudio`, and `torchvision`. Subsequently, we install the PyTorch nightly version along with its dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c14RQjy8A6MH"
      },
      "source": [
        "To further enrich our toolkit, we install several additional libraries that play important roles in different stages of the ASR pipeline. These include:\n",
        "\n",
        "- **Data Processing**: `sox` (audio processing), `scrapy` (data scraping), `ICU-Tokenizer` (text tokenization), `datasets` (loading training data), `librosa` (audio processing).\n",
        "\n",
        "- **Model Training and Evaluation**: `transformers` (training the model), `evaluate` (assessing model performance), `jiwer` (calculating Word Error Rate).\n",
        "\n",
        "- **Efficiency and Optimization**: `accelerate` (for faster training and evaluation), `peft` (lightweight training using LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O-nLC2sN3UC"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torch torchaudio torchvision -y # we need to install the nightly version of torch\n",
        "!pip install -q --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
        "!pip install -q fairseq # we will use this package for aligning the audio-text\n",
        "!pip install -q dataclasses\n",
        "!pip install -q sox # for audio processing\n",
        "!pip install -q scrapy # for scapping the data\n",
        "!pip install -q ICU-Tokenizer # for tokenizing the text\n",
        "!pip install -q transformers # we will use huggingface transformers for training the models\n",
        "!pip install -q datasets # we will use huggingface datasets for loading the training dataset\n",
        "!pip install -q librosa # required by huggingface dataset\n",
        "!pip install -q evaluate # for evaluating the models\n",
        "!pip install -q jiwer # for computing WER (Word Error Rate)\n",
        "!pip install -q bitsandbytes # for loading the quantized model\n",
        "!pip install -q accelerate # for efficient training and evaluation\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main # for lightweight training using LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2oC8fZIA6MI"
      },
      "source": [
        "In this step, we clone repositories containing code and resources essential for our ASR project. Specifically, we clone the `cawoylel/nlp4all` repository, which holds the code for this tutorial, and the `isi-nlp/uroman` repository, which provides functionalities for Romanization of text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty-GLgwQ_VOn"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cawoylel/nlp4all.git # repository containing the code of this tutorial\n",
        "!git clone https://github.com/isi-nlp/uroman.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYVw62egXVtj"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "git clone https://github.com/facebookresearch/fairseq.git\n",
        "cd fairseq\n",
        "pip install --editable ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otwu4xHzA6MJ"
      },
      "source": [
        "Finally, we specify the model name, task, language, and language abbreviation for our ASR model. Since the Whisper model does not directly support Seerer or Fula languages, we employ a workaround by selecting the closest available language within the model. Several strategies can be used to select the best language for fine-tuning. For Fula for instance, we selected **Hausa** language. This selection process can involve:\n",
        "\n",
        "- Choosing the language with the best score on the Fula development corpus.\n",
        "- Choosing the language that the model most frequently predicts for Fula/Seerer audio.\n",
        "- Choosing the language that tokenizes Fula/Seerer texts most effectively\n",
        "\n",
        "Language selection ensures that our ASR model operates effectively despite the absence of direct language support."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLLeYERBH80s"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"openai/whisper-large-v3\"\n",
        "task = \"transcribe\"\n",
        "language = \"Hausa\"\n",
        "language_abbr = \"ha\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hkzQTtLLT2v"
      },
      "source": [
        "# Getting the ASR data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you may know, data is one of the most fundamental component of artificial intelligence. For many African languages like **Seerer** or **Fula**, finding enough high-quality data for AI to learn from can feel like searching for buried treasure! Data scarcity is a major roadblock for developing Natural Language Processing (NLP) tools for these languages\n",
        "\n",
        "To overcome this challenge, we've turned to the **Bible**. It has been translated into a countless of languages, ncluding those with limited online resources like Seerer. This source is immensely valuable as it not only provides text data but also includes audio recordings. These recordings are made by native speakers, offering authentic insights into pronunciation, intonation, and linguistic nuances.\n",
        "\n",
        "But, there are a couple of things to keep in mind. First, these recordings are usually done in very quiet, studio-like settings where people speak very clearly. While this ensures high-quality audio, models trained on these data might struggle in noisier, real-world conditions where speech patterns are more varied and spontaneous. Additionally, the majority of these recordings are by adult males, which could limit the model's ability to accurately recognize voices of other genders.\n",
        "\n",
        "Acknowledging these limitations, we can for instance employed data augmentation techniques to enhance the robustness of our models. By introducing elements like background noise into the dataset, we can simulate more diverse listening environments, thereby preparing our models to perform reliably in a variety of real-world scenarios.\n",
        "\n",
        "The Seerer Bible recordings might not be perfect, but for a language with limited online resources, they provide a starting point to building powerful ASR tools.\n"
      ],
      "metadata": {
        "id": "b7cvWIftBBe0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uv-NfY6Ljre"
      },
      "source": [
        "## Bible Crawler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section imports the required modules for the scraping process. It includes `CrawlerProcess` from Scrapy for managing the crawling process, `SentSplitter` from `icu_tokenizer` for sentence splitting, and the `BibleScraper` we developed. You can find the code in `scraper.py` in `src` folder.\n",
        "\n"
      ],
      "metadata": {
        "id": "btRVnCiyQW7w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5stE19ySKhO4"
      },
      "outputs": [],
      "source": [
        "from scrapy.crawler import CrawlerProcess\n",
        "from icu_tokenizer import SentSplitter\n",
        "from nlp4all.asr.src.scraper import BibleScraper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We initialize the `SentSplitter` object."
      ],
      "metadata": {
        "id": "cRZfDfoqUgPY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8psgKPNL8gq"
      },
      "outputs": [],
      "source": [
        "SPLITTER = SentSplitter()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This block of code sets up and executes the scraping process using Scrapy. It initializes the `CrawlerProcess` object, specifies parameters for the scraping task (such as the name of the scraper, output folder where to store the data, start URLs, language, and code), and then starts the scraping process."
      ],
      "metadata": {
        "id": "z5KgqT2uVB6U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJuuwbiaMEuR"
      },
      "outputs": [],
      "source": [
        "process = CrawlerProcess()\n",
        "process.crawl(BibleScraper,\n",
        "              name=\"SeereerBible\",\n",
        "              output_folder=\"SeereerBible\",\n",
        "              start_urls=[\"https://www.bible.com/bible/3751/GEN.1.SRR23\"], # You can change this according\n",
        "              language=\"Sereer-Sine\",\n",
        "              code=\"SRR23\",\n",
        "              splitter=SentSplitter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze7dOj6Ose5k"
      },
      "outputs": [],
      "source": [
        "process.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXa7lmZ-CV0s"
      },
      "source": [
        "## Resampling the audios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyZtr8YXCaBR"
      },
      "source": [
        "After downloading the audios, we need to resample the audios. Many modern speech models only deal with 16k sampling. We will use ffmpeg to resample the audios into 16k and also to convert them to .wav file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQOR_M2AQqDQ"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "for f in /content/SeereerBible/raw/Sereer-Sine/*.mp3; do\n",
        "  filename=\"$(basename \"$f\")\"\n",
        "  directory=\"$(dirname \"$f\")\"\n",
        "  stem=${filename%.*}\n",
        "  ffmpeg -i $f -ac 1 -ar 16000 $directory/$stem.wav ;\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHPQxYv3M9oq"
      },
      "source": [
        "## Neural Forced Alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHqkn2_OCz4Y"
      },
      "source": [
        "As such, we cannot use the data in order to train an ASR model. Because the audios are too long. We ideally need segment less than 30s long and the corresponding text.\n",
        "\n",
        "Splitting the audios into short segments while finding the corresponding transcription is what called \"forced alignment\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOGlte_SOe8T"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "input_folder=/content/SeereerBible/raw/Sereer-Sine\n",
        "output_folder=/content/SeereerBible/aligned/Sereer-Sine\n",
        "cd fairseq/\n",
        "for text in $input_folder/*.txt; do\n",
        "  filename=\"$(basename \"$text\")\"\n",
        "  stem=${filename%.*}\n",
        "  output_path=$output_folder/$stem\n",
        "  rm -rf $output_path\n",
        "  python -m examples.mms.data_prep.align_and_segment \\\n",
        "  --audio_filepath $input_folder/$stem.wav \\\n",
        "  --text_filepath $input_folder/$stem.txt \\\n",
        "  --lang ful \\\n",
        "  --outdir $output_path \\\n",
        "  --uroman /content/uroman/bin\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6RMLHQ6Ilie"
      },
      "source": [
        "## Preparing the HuggingFace dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYqmxNZalBwU"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.disable(logging.DEBUG)\n",
        "logging.disable(logging.INFO)\n",
        "logging.disable(logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAwDGUKQJmac"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import datasets\n",
        "# import logging\n",
        "# numba_logger = logging.getLogger('numba')\n",
        "# numba_logger.setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOyRZtxKU4c_"
      },
      "outputs": [],
      "source": [
        "features = datasets.Features(\n",
        "    {\n",
        "        \"audio\": datasets.features.Audio(sampling_rate=16_000),\n",
        "        \"transcription\": datasets.Value(\"string\"),\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEJPvsbb7-cF"
      },
      "outputs": [],
      "source": [
        "def manifest_to_folder(input_folder: str):\n",
        "    input_folder = Path(input_folder)\n",
        "    for chapter in input_folder.glob(\"*/\"):\n",
        "        with open(chapter / \"manifest.json\", \"r\") as manifest:\n",
        "            for line in manifest:\n",
        "                data = eval(line)\n",
        "                audio_filepath = data[\"audio_filepath\"]\n",
        "                text = data[\"text\"]\n",
        "                yield {\n",
        "                    \"audio\": audio_filepath,\n",
        "                    \"transcription\": text\n",
        "                }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3_z7sSzKdnE"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.Dataset.from_generator(manifest_to_folder,\n",
        "                                          features=features,\n",
        "                                          gen_kwargs={\"input_folder\": \"/content/SeereerBible/aligned/Sereer-Sine\"}\n",
        "                                          ).cast_column(\"audio\", datasets.Audio())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xby4myb3Hv6U"
      },
      "source": [
        "## Preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqOo5RQaHyK5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import WhisperFeatureExtractor\n",
        "from transformers import WhisperTokenizer\n",
        "from transformers import WhisperProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B7IwNKFIAf4"
      },
      "outputs": [],
      "source": [
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pQCCFRmKjuc"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9CBdTQOKrCb"
      },
      "outputs": [],
      "source": [
        "num_proc = os.cpu_count()\n",
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names, num_proc=num_proc)\n",
        "dataset = dataset.train_test_split(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHsygYJiaWNt"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghpi9gDjB28x"
      },
      "source": [
        "# Finetune Whisper Large on Seereer ASR dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7fpfTjGMlNp"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsxblQujMzuS"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvizn71iMvkg"
      },
      "outputs": [],
      "source": [
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMWTE0jaNx7b"
      },
      "source": [
        "## Prepare the pretrained model for LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siySu0eQM_cl"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "model.model.encoder.conv1.register_forward_hook(lambda module, input, output: output.requires_grad_(True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8nLlYxxN0sX"
      },
      "source": [
        "## Attach adapters to the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU7Opfu2N_jP"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaPV7-AmPoTy"
      },
      "outputs": [],
      "source": [
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74MzKS0HPXoX"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdKKAckZMSs-"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IlRuDIsMVAj"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAiiH2WRPdnL"
      },
      "source": [
        "## Training arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECWQpHN0Pfaw"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    logging_steps=100,\n",
        "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM4xi3BZQzAq"
      },
      "source": [
        "## Launch the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Xh_Rc0XBH4"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cZAE8VgQ1JI"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZovwoAU5Q3zn"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKFw3-rIyJMg"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34jpYPFjgnlw"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "eval_dataloader = DataLoader(dataset[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=32,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
        "\n",
        "print(f\"{wer=} and {normalized_wer=}\")\n",
        "print(eval_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiieDIAZyPL5"
      },
      "outputs": [],
      "source": [
        "list(zip(predictions, references))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-jCBsbWyxSw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}