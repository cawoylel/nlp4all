{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cawoylel/nlp4all/blob/main/asr/src/asr_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FffJ6hD7dVqw"
      },
      "source": [
        "---\n",
        "\n",
        "<h1 align=\"center\"><strong>Developing a Speech Recognition System for your language: a pratical guide from data acquisition to model training</strong></h1>\n",
        "\n",
        "<h4 align=\"center\"><strong>Yaya Sy, Dioula DoucourÃ©</strong></h4>\n",
        "\n",
        "---\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/cawoylel/nlp4all/blob/main/asr/illustrations/cawoylel.png?raw=true:, width=200\" alt=\"transformer\" width=200>\n",
        "<br>\n",
        "    <em>\n",
        "    https://cawoylel.com/\n",
        "    </em>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jKa71Y6_VYK"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfrP1cKUDhAo"
      },
      "source": [
        "Few months ago, we released __windanam__, a collection of multidialectal speech recognition models for the Fula language, which is underrepresented in NLP solutions. During the development phase, we encountered significant hurdles in gathering data and creating the models. The task of developing Speech Recognition Systems for such languages presents unique challenges. The process of Automatic Speech Recognition (ASR), which relies on a supervised learning approach, necessitates the availability of annotated datasets. These datasets must include both the audio recordings and their corresponding textual transcriptions. However, acquiring this type of data is particularly arduous for languages that are predominantly spoken and either lack a written form or have an unstandardized script.\n",
        "\n",
        "In this guide, we outline the foundational steps necessary for developing an ASR model for languages that do not have pre-existing annotated datasets for ASR. We detail the strategies and methodologies that contributed to the successful creation of __windanam__, covering everything from the initial data collection phase to the model training stage. This guide marks the beginning of our [_NLP4ALL_](https://github.com/cawoylel/nlp4all) series, which is focused on simplifying the process of building NLP models for underrepresented languages and making it more accessible. Our initiative is not limited to just one language; it embodies a broader mission to promote open-source collaboration and democratize NLP technology for every language. We aim to provide a replicable framework that communities can adapt for their languages, aligning with our vision of making NLP technology widely accessible.\n",
        "\n",
        "While this tutorial uses the Seereer language as a case study, the outlined process is applicable to any language facing similar challenges of underrepresentation in the NLP domain. Notably, there is a lack of annotated datasets for the development of ASR models in Seereer. By selecting Seereer as our example, we dissect the entire workflow, from the generation of data to the training of the model. This includes steps such as audio-speech scraping, short segments alignment, data pocessing, and the culmination of building your ASR model through the fine-tuning of an existing open-source model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkBwANXMA6MB"
      },
      "source": [
        "# Table of Contents\n",
        "\n",
        "1. [Setup and Configuration](#setup)\n",
        "2. [Getting the ASR Data](#data)\n",
        "    - 2.1 [Bible Crawler](#crawler)\n",
        "    - 2.2 [Resampling the Audios](#resampling)\n",
        "    - 2.3 [Neural Forced Alignment](#aligner)\n",
        "3. [Data Preprocessing](#datasets)\n",
        "4. [Fine-Tuning Whisper Large on Seereer ASR Dataset](#finetuning)\n",
        "    - 4.1 [Data processing](#preprocess)\n",
        "    - 4.1 [Load Model](#load)\n",
        "    - 4.2 [Prepare the Pretrained Model for LoRA](#prepare)\n",
        "    - 4.3 [Attach Adapters to the Pretrained Model](#attach)\n",
        "    - 4.4 [Data Loader](#collator)\n",
        "    - 4.5 [Training Arguments](#training_args)\n",
        "    - 4.6 [Launch Training and Evaluate](#training)\n",
        "5. [Evaluation](#evaluate)\n",
        "6. [Challenges and future directions](#challenges)\n",
        "    - 6.1 [Robustness](#robust)\n",
        "    - 6.2 [Lexical Diversity](#vocab)\n",
        "    - 6.3 [Deploy your model](#deploy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZM5ZQKY_SEJ"
      },
      "source": [
        "# Setup  <a name=\"setup\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYPVBNTUA6MD"
      },
      "source": [
        "We use Google Colab Notebook in this tutorial, which is a free Ubuntu Virtual Machine with free CPUs (2) provided by Google. This Virtual Machin also contains some free hours of a single 16GB GPU.\n",
        "\n",
        "Let a setup quickly the Colab Virtual Machine to ensure that our environment has all the necessary dependencies installed. First, we update the package list with `apt-get update`. Then, we install essential packages required for audio processing, such as `libsox-fmt-all`, `sox`, and `ffmpeg`. Additionally, we install `libicu-dev` and `pkg-config` to handle text processing and Unicode symbols effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwpHPRFVs032"
      },
      "outputs": [],
      "source": [
        "!apt install libicu-dev pkg-config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddzlqgyTNrVB"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install libsox-fmt-all sox ffmpeg # needed for processing audio\n",
        "!apt install libicu-dev pkg-config # needed for processing text and unicode symbols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIR7MAujA6MG"
      },
      "source": [
        "Let now setup the Python environment. We focus on installing the PyTorch nightly version, which is crucial for compatibility with other libraries and tools. We begin by uninstalling existing versions of `torch`, `torchaudio`, and `torchvision`. Subsequently, we install the PyTorch nightly version along with its dependencies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c14RQjy8A6MH"
      },
      "source": [
        "To further enrich our toolkit, we install several additional libraries that play important roles in different stages of the ASR pipeline. These include:\n",
        "\n",
        "- **Data Processing**: `sox` (audio processing), `scrapy` (data scraping), `ICU-Tokenizer` (text tokenization), `datasets` (loading training data), `librosa` (audio processing).\n",
        "\n",
        "- **Model Training and Evaluation**: `transformers` (training the model), `evaluate` (assessing model performance), `jiwer` (calculating Word Error Rate).\n",
        "\n",
        "- **Efficiency and Optimization**: `accelerate` (for faster training and evaluation), `peft` (lightweight training using LoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR-bQ5cBtSH1"
      },
      "outputs": [],
      "source": [
        "!apt install libicu-dev pkg-config\n",
        "!pip install -q ICU-Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7NtvpryyyZV"
      },
      "outputs": [],
      "source": [
        "!pip install -q scrapy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O-nLC2sN3UC"
      },
      "outputs": [],
      "source": [
        "!pip uninstall torch torchaudio torchvision -y # we need to install the nightly version of torch\n",
        "!pip install -q --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
        "!pip install -q fairseq # we will use this package for aligning the audio-text\n",
        "!pip install -q dataclasses\n",
        "!pip install -q sox # for audio processing\n",
        "!pip install -q scrapy # for scapping the data\n",
        "!pip install -q ICU-Tokenizer # for tokenizing the text\n",
        "!pip install -q transformers # we will use huggingface transformers for training the models\n",
        "!pip install -q datasets # we will use huggingface datasets for loading the training dataset\n",
        "!pip install -q librosa # required by huggingface dataset\n",
        "!pip install -q evaluate # for evaluating the models\n",
        "!pip install -q jiwer # for computing WER (Word Error Rate)\n",
        "!pip install -q bitsandbytes # for loading the quantized model\n",
        "!pip install -q accelerate # for efficient training and evaluation\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main # for lightweight training using LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2oC8fZIA6MI"
      },
      "source": [
        "In this step, we clone repositories containing code and resources essential for our ASR project. Specifically, we clone the `cawoylel/nlp4all` repository, which holds the code for this tutorial, and the `isi-nlp/uroman` repository, which provides functionalities for Romanization of text data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty-GLgwQ_VOn"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/cawoylel/nlp4all.git # repository containing the code of this tutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%shell\n",
        "git clone https://github.com/isi-nlp/uroman.git\n",
        "git checkout 7750feb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYVw62egXVtj"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "git clone https://github.com/facebookresearch/fairseq.git\n",
        "cd fairseq\n",
        "pip install --editable ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hkzQTtLLT2v"
      },
      "source": [
        "# Getting the ASR data  <a name=\"data\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7cvWIftBBe0"
      },
      "source": [
        "As we said earlier, obtaining ASR data for training is really difficult for many languages. Data scarcity is a major roadblock in developing Natural Language Processing (NLP) tools for underrepresented languages.\n",
        "\n",
        "To train a Speech Recognition model, you need a pair of spoken utterances and the corresponding textual transcription. How do you find this data? One good resource is the Common Voice project, a data collection initiative where speakers record themselves reading sentences. To date, it includes over a hundred languages, and the language you're interested in may be among them. Unfortunately, Sereer is not included.\n",
        "\n",
        "If your language doesn't have annotated data for ASR, maybe you can find some audiobooks with their transcriptions. One audiobook resource that covers many languages is the Bible. This book has been translated into more than 4,000 languages in audiobook form. These recordings are made by native speakers, offering authentic insights into pronunciation, intonation, and linguistic nuances. However, we only have transcriptions for 1,000 languages. This is really a valuable resource for many languages, and perhaps the language you are interested in is included. You can check for this on the following website: https://www.bible.com/. Fortunately, the Seereer language has audiobooks and the corresponding transcriptions, for example, for the book of Samuel: https://www.bible.com/bible/3751/1SA.1.SRR23.\n",
        "\n",
        "However, there are a couple of things to keep in mind. First, these recordings are usually done in very quiet, studio-like settings where people speak very clearly. While this ensures high-quality audio, models trained on these data might struggle in noisier, real-world conditions where speech patterns are more varied and spontaneous. Additionally, the majority of these recordings are by adult males, which could limit the model's ability to accurately recognize voices of other genders. The Seereer Bible recordings might not be perfect, but for a language with limited online resources, they provide a starting point for building powerful ASR tools."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Uv-NfY6Ljre"
      },
      "source": [
        "## Bible Crawler <a name=\"crawler\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey4tVjZ_mSqZ"
      },
      "source": [
        "Let scrap the bible data for Seereer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btRVnCiyQW7w"
      },
      "source": [
        "This section imports the required modules for the scraping process. It includes `CrawlerProcess` from Scrapy for managing the crawling process, `SentSplitter` from `icu_tokenizer` for sentence splitting, and the `BibleScraper` we developed. You can find the code in `scraper.py` in the `asr/src` folder of the `nlp4all` repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5stE19ySKhO4"
      },
      "outputs": [],
      "source": [
        "from scrapy.crawler import CrawlerProcess\n",
        "from icu_tokenizer import SentSplitter\n",
        "from nlp4all.asr.src.scraper import BibleScraper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRZfDfoqUgPY"
      },
      "source": [
        "We initialize the `SentSplitter` object. You can also create your own splitter class that implement a method `.split()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8psgKPNL8gq"
      },
      "outputs": [],
      "source": [
        "SPLITTER = SentSplitter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5KgqT2uVB6U"
      },
      "source": [
        "This block of code sets up and executes the scraping process using Scrapy. It initializes the CrawlerProcess object, specifies parameters for the scraping task (such as the name of the scraper, the output folder where the data will be stored, start URLs, language, and language code), and then starts the scraping process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJuuwbiaMEuR"
      },
      "outputs": [],
      "source": [
        "process = CrawlerProcess()\n",
        "process.crawl(BibleScraper,\n",
        "              name=\"SeereerBible\", # the name of scraping process, you can choose any name you want, it's not an important argument.\n",
        "              output_folder=\"SeereerBible\", # Where to store the scraped data.\n",
        "              start_urls=[\"https://www.bible.com/bible/3751/GEN.1.SRR23\"], # Look at the bible website and copy here the link of the first chapter of the first book\n",
        "              language=\"Sereer-Sine\", # TODO: remove some arguments to make this simple to understand\n",
        "              code=\"SRR23\",\n",
        "              splitter=SentSplitter())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ze7dOj6Ose5k"
      },
      "outputs": [],
      "source": [
        "process.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXa7lmZ-CV0s"
      },
      "source": [
        "## Resampling the audios  <a name=\"resampling\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyZtr8YXCaBR"
      },
      "source": [
        "After downloading the audios, we need to resample them. Many modern speech models only deal with *16 000 sampling*. We will use `ffmpeg` to resample the audios into 16 000. We will also save the resampled audios into `.wav` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQOR_M2AQqDQ"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "for f in /content/SeereerBible/raw/Sereer-Sine/*.mp3; do\n",
        "  filename=\"$(basename \"$f\")\"\n",
        "  directory=\"$(dirname \"$f\")\"\n",
        "  stem=${filename%.*}\n",
        "  ffmpeg -i $f -ac 1 -ar 16000 $directory/$stem.wav ;\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHPQxYv3M9oq"
      },
      "source": [
        "## Neural Forced Alignment  <a name=\"aligner\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHqkn2_OCz4Y"
      },
      "source": [
        "As such, we cannot directly train a Speech Recognition model on an entire audiobook because the audio is too long to be fed into a model. Ideally, we need smaller segments (perhaps 30 seconds each). Splitting the audio while keeping it aligned with the corresponding transcription is not an easy task. We can achieve this by using a forced aligner, a method that aligns short short spoken utterances with their corresponding transcriptions.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/cawoylel/nlp4all/blob/main/asr/illustrations/forced_aligner.png?raw=true:, width=200\" alt=\"transformer\" width=500 class=\"center\">\n",
        "<br>\n",
        "    <em>\n",
        "    Illustration of the task of Forced Alignement\n",
        "    </em>\n",
        "</p>\n",
        "\n",
        "We will use the [MMS](https://github.com/facebookresearch/fairseq/blob/main/examples/mms/README.md) Forced Aligner to do this. This is a Forced Aligner using a multilingual speech model trained on thousand of languages. You can check here if your language is included: https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html\n",
        "\n",
        "If you cannot find your language, you can also choose the language the more similar to yours, this can also give good results. In the case of the Seereer language, we will use the Fula model (code `ful`) to align the Seereer data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzxR7qQNkw8x"
      },
      "source": [
        "Note: If you use google colab, you may see some errors like `cannot open shared object file: No such file or director` or `Failed to load FFmpeg5 extension` while running the following cell. Just ignore them as it will not affect the alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOGlte_SOe8T"
      },
      "outputs": [],
      "source": [
        "%%shell\n",
        "input_folder=/content/SeereerBible/raw/Sereer-Sine\n",
        "output_folder=/content/SeereerBible/aligned/Sereer-Sine\n",
        "cd fairseq/\n",
        "for audio in $input_folder/*.wav; do\n",
        "  filename=\"$(basename \"$audio\")\"\n",
        "  stem=${filename%.*}\n",
        "  output_path=$output_folder/$stem\n",
        "  rm -rf $output_path\n",
        "  python -m examples.mms.data_prep.align_and_segment \\\n",
        "  --audio_filepath $input_folder/$stem.wav \\\n",
        "  --text_filepath $input_folder/$stem.txt \\\n",
        "  --lang ful \\\n",
        "  --outdir $output_path \\\n",
        "  --uroman /content/uroman/bin\n",
        "done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6RMLHQ6Ilie"
      },
      "source": [
        "# Preparing the HuggingFace dataset  <a name=\"datasets\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIcuquZZxm0w"
      },
      "source": [
        "Once you've downloaded the audio data, split it into shorter segments (less than 30 seconds) and performed neural forced alignment to obtain the corresponding transcripts (as explained in the Neural Forced Alignment section), we'll use **Hugging Face Datasets** to create a structured data format for training our ASR model.\n",
        "\n",
        "If you're unfamiliar with ðŸ¤— [Datasets](https://huggingface.co/docs/datasets/en/index), it's a library provided by Hugging Face for processing large scale corpora. You ca use this to load the datasets available on the Huggingface Hub (text corpora, classification datasets, translation datasets, question answering datasets, and more). You can also just use the Datasets library as a python API for processing efficiently your data on your local machine. In our case, we'll leverage Hugging Face Datasets to process and organize our Seereer speech data and transcripts into a format readily usable for training our ASR model.\n",
        "\n",
        "We will first import the `logging` library and disable unnecessary logging messages to keep the console output cleaner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYqmxNZalBwU"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logging.disable(logging.DEBUG)\n",
        "logging.disable(logging.INFO)\n",
        "logging.disable(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTU-megVy6_L"
      },
      "source": [
        "We then import the `Path` class from the `pathlib` library, which provides a convenient way to handle file paths. We also import the `datasets` library from Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAwDGUKQJmac"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goHv05rezZvE"
      },
      "source": [
        "We define the structure of our dataset using the **`Features`** class from the Hugging Face datasets module. This structure specifies the format of each sample in the dataset. It includes two key elements:\n",
        "\n",
        "- **audio**: This defines an audio feature with a sampling rate of 16 kHz, which is a common standard for speech model.\n",
        "\n",
        "- **transcription**: This defines a string feature to store the corresponding text transcript for each audio segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOyRZtxKU4c_"
      },
      "outputs": [],
      "source": [
        "features = datasets.Features(\n",
        "    {\n",
        "        \"audio\": datasets.features.Audio(sampling_rate=16_000),\n",
        "        \"transcription\": datasets.Value(\"string\"),\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRBv6QuLz7X1"
      },
      "source": [
        "We define a function called **`dataset_generator`** that takes the path to the input folder (containing the processed audio data and transcripts) as input. This function essentially reads information about each audio-text pair from the manifest files and prepares it in a format suitable for the dataset creation process.\n",
        "\n",
        "The function iterates through each bible chapter subfolder and reads the corresponding **manifest.json** file that contains information about each audio segment and its transcript. We extract the audio file path and text transcript from each line in the manifest file. Finally, the function  yields a dictionary with two key-value pairs: **audio** containing the audio file path and **transcription** containing the text transcript."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEJPvsbb7-cF"
      },
      "outputs": [],
      "source": [
        "def dataset_generator(input_folder: str):\n",
        "    input_folder = Path(input_folder)\n",
        "    for chapter in input_folder.glob(\"*/\"):\n",
        "        with open(chapter / \"manifest.json\", \"r\") as manifest:\n",
        "            for line in manifest:\n",
        "                data = eval(line)\n",
        "                audio_filepath = data[\"audio_filepath\"]\n",
        "                text = data[\"text\"]\n",
        "                yield {\n",
        "                    \"audio\": audio_filepath,\n",
        "                    \"transcription\": text\n",
        "                }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdopZWwL1W3u"
      },
      "source": [
        "Our final step is to create the Hugging Face Dataset using `datasets.Dataset.from_generator`.\n",
        "\n",
        "`from_generator` module in Hugging Face Datasets provides a convenient way to create datasets from Python generators, enabling efficient handling of large or dynamically generated datasets in machine learning projects\n",
        "\n",
        "\n",
        "\n",
        "> ðŸ¤—\n",
        "The `from_generator()` method is the most memory-efficient way to create a dataset from a generator due to a generators iterative behavior. This is especially useful when youâ€™re working with a really large dataset that may not fit in memory, since the dataset is generated on disk progressively and then memory-mapped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3_z7sSzKdnE"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.Dataset.from_generator(dataset_generator,\n",
        "                                          features=features,\n",
        "                                          gen_kwargs={\"input_folder\": \"/content/SeereerBible/aligned/Sereer-Sine\"}\n",
        "                                          ).cast_column(\"audio\", datasets.Audio())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXklqGfO16YK"
      },
      "source": [
        "`cast_column(\"audio\", datasets.Audio())` method ensures the **audio** column is correctly recognized as audio data by Hugging Face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZPYceLK2hhR"
      },
      "source": [
        "Well, now we have short text sentences with their corresponding audio files in the format of HuggingFace dataset. We're ready to process for a specific model. In this tutorial, we will use Whisper, a pretrained multilingual ASR model. Let see how to prepare the data for this specific model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzVrw3_qP4Fa"
      },
      "source": [
        "# Finetuning Whisper model on the Seereer data <a name=\"finetuning\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U-_4-yAQJlX"
      },
      "source": [
        "Whisper is an encoder-decoder model designed for speech processing. The encoder, a Transformer, is trained to represent speech effectively, while the decoder, also a Transformer, is tasked with transcribing the speech based on these representations. Both components are trained simultaneously: the encoder learns to provide accurate representations to the decoder, which in turn learns to generate transcriptions from these representations. Whisper is a multilingual and multitask model, supporting approximately a hundred languages. Beyond speech recognition, it is capable of performing additional tasks, such as translating spoken input. We plan to adapt this model to work with Seereer data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G28_MZKpP_uY"
      },
      "source": [
        "Since the Whisper model does not directly support Seerer or Fula languages, we employ a workaround by selecting the closest available language within the model. Several strategies can be used to select the best language for fine-tuning. For Seereer for instance, we selected **Hausa** language. This selection process can involve:\n",
        "\n",
        "- Choosing the language with the best score on a Seereer development corpus.\n",
        "- Choosing the language that the model most frequently predicts for the Seerer audios.\n",
        "- Choosing the language that tokenizes Seereer texts most effectively.\n",
        "\n",
        "Language selection ensures that our ASR model operates effectively despite the absence of direct language support.\n",
        "We will use the Hausa language, as we found that this language works well for Fula in previous experiments, and we know that Fula is linguistically the closest language to Seereer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ0g44JvQF3j"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"openai/whisper-large-v3\"\n",
        "task = \"transcribe\"\n",
        "language = \"Hausa\"\n",
        "language_abbr = \"ha\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xby4myb3Hv6U"
      },
      "source": [
        "## Preprocess the dataset for the Whisper model <a name=\"preprocess\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBKZ_6wE4KPA"
      },
      "source": [
        "As the model is an encoder-decoder, and that the encoder takes the audio as input and the decoder takes the text as output, we need two processors. The first will process the speech the second will process the text. In NLP, a text processor is just a `Tokenizer` a simple model that split the text into smaller pieces of words and subwords. For the speech processor, it depends on the model. In the case of Whisper, the speech encoder takes as input a Spectogram (an image representation of the audio). So the audio processor will transform the raw speech audio into spectogram. This operation is called `FeatureExtraction` here. The transformer library gives a convient way of using these two processors: the `Processor` class encapsulates both in a single class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqOo5RQaHyK5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import WhisperFeatureExtractor\n",
        "from transformers import WhisperTokenizer\n",
        "from transformers import WhisperProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B7IwNKFIAf4"
      },
      "outputs": [],
      "source": [
        "# feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "# tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
        "tokenizer = processor.tokenizer\n",
        "feature_extractor = processor.feature_extractor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2q-03v198qeR"
      },
      "source": [
        "This function will extract speech features of the audio and tokenize the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pQCCFRmKjuc"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9CBdTQOKrCb"
      },
      "outputs": [],
      "source": [
        "num_proc = os.cpu_count() # Parallelize the process using multiple CPUs\n",
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names, num_proc=num_proc)\n",
        "dataset = dataset.train_test_split(0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHsygYJiaWNt"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1iTf2ay-rv9"
      },
      "source": [
        "Now, we're officially done with the data preparation part!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghpi9gDjB28x"
      },
      "source": [
        "## Load Whisper model <a name=\"load\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fMy4KKa-p_R"
      },
      "source": [
        "We have the data already preprocessed and ready for training. Now we will load the model and prepare it for training. We will use QLoRa, which is an efficient method used for training large transformer models without the need of big compute ressources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7fpfTjGMlNp"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvXpG8lWCmpf"
      },
      "source": [
        "We load the Whisper model from the HuggingFace hub. Instead of loading the model in full precision, we load it in 8bit so it can fit easily on a single small GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvizn71iMvkg"
      },
      "outputs": [],
      "source": [
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VfIdG48KsXi"
      },
      "source": [
        "Training in a 8bit is special compared to training a model in full (FP32) or half (fp16) precision. So we use the Transformer library to prepare the model for 8bit training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siySu0eQM_cl"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "model.model.encoder.conv1.register_forward_hook(lambda module, input, output: output.requires_grad_(True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8nLlYxxN0sX"
      },
      "source": [
        "## Attach LoRA adapters to the pretrained model <a name=\"attach\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DaqVO4zKqSQ"
      },
      "source": [
        "Instead of updating all the model parameters like conventional finetuning, we attach small adapters with few parameters to the model, and only these parameters are updated during finetuning. We will use Lora for this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU7Opfu2N_jP"
      },
      "outputs": [],
      "source": [
        "config = LoraConfig(r=32, # The rank to use. A big rank will require more GPU memory\n",
        "                    lora_alpha=64, # used for scaling. Generaly alpha = r // 2\n",
        "                    target_modules=[\"q_proj\", \"v_proj\"], # which modules of the transformer to adapt.\n",
        "                    lora_dropout=0.05,\n",
        "                    bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74MzKS0HPXoX"
      },
      "source": [
        "## Datacollator <a name=\"collator\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEaope2KLCW1"
      },
      "source": [
        "This class is called when iterating over the batches. It will pad the inputs, mask the padded tokens so the model will not learn to generate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdKKAckZMSs-"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IlRuDIsMVAj"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAiiH2WRPdnL"
      },
      "source": [
        "## Training arguments <a name=\"training_args\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYjwx_8EPGot"
      },
      "source": [
        "Set your training arguments as you want. Check the arguments for the class [TrainingArguments](https://huggingface.co/docs/transformers/v4.39.1/en/main_classes/trainer#transformers.TrainingArguments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECWQpHN0Pfaw"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"nlp4all/seereer_whisper\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    logging_steps=10,\n",
        "    save_steps=20,\n",
        "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaPV7-AmPoTy"
      },
      "outputs": [],
      "source": [
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zM4xi3BZQzAq"
      },
      "source": [
        "## Launch the training <a name=\"training\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6Xh_Rc0XBH4"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"wer\") # we will use the Word Error Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cZAE8VgQ1JI"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZovwoAU5Q3zn"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnzmUmjvkWst"
      },
      "source": [
        "After training, the model is saved in your local machine so you can use it later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt1laphYgrKX"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"Seereer_Bible_100steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKFw3-rIyJMg"
      },
      "source": [
        "# Evaluation <a name=\"evaluate\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFqXHiZbmAWl"
      },
      "source": [
        "Once training is done, you can evaluate your model on the test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yw3wD7xunolt"
      },
      "outputs": [],
      "source": [
        "from nlp4all.asr.src.evaluate import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLSs5CW-6gok"
      },
      "outputs": [],
      "source": [
        "results = evaluate(processor, dataset, data_collator, language, task, metric, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6_-IHy58PPV"
      },
      "source": [
        "Just a small comparaison between predicted and references transcriptions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pa-d9Jm7526"
      },
      "outputs": [],
      "source": [
        "print(list(zip(results[\"unnormalized_predictions\"],\n",
        "               results[\"unnormalized_references\"])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFmUjzV7noaO"
      },
      "source": [
        "**...**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE2u0oiHlvXP"
      },
      "source": [
        "# Challenges and future directions <a name=\"challenges\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahtzfjq241E6"
      },
      "source": [
        "## Robustness <a name=\"robust\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4II65s_ly7Y"
      },
      "source": [
        "We used the Bible data. As said in the *Data Collection* section, there are some limitations that should be taken into account when using for instance the Bible in your ASR pipeline (high quality audio recorded in ideal situation without any background noise, predominent male voices, and so on). Acknowledging these limitations, we can for instance employ **data augmentation** techniques to enhance the robustness of our models. By introducing elements like background noise into the dataset, we can simulate more diverse listening environments, thereby preparing our models to perform reliably in a variety of real-world scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNypFAce5Mc4"
      },
      "source": [
        "## Lexical diversity <a name=\"vocab\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F0wv6K75Or-"
      },
      "source": [
        "The Bible also employs a limited vocabulary, which is distinctly religious and different from words used in everyday life. If your language has texts from domains other than religious, you can attempt to synthesize these texts using a pretrained model like MMS or by training your own Text-to-Speech model using the data we've generated in this tutorial. Additionally, you can explore adversarial techniques to adapt your Whisper model to other diverse text sources (see this [paper](https://isl.anthropomatik.kit.edu/downloads/Text%20and%20Synthetic%20Data%20for%20Domain%20Adaptation%20in%20End-to-End%20Speech%20Recognition.pdf) for an example)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GovRXuhAyPei"
      },
      "source": [
        "## Deploy your model <a name=\"deploy\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKHaDLSSyWvH"
      },
      "source": [
        "Indeed, for your model to be useful, you must publish it and create applications that Seereer speakers can use and interact with.\n",
        "\n",
        "You can deploy your model for free on CPU instances at HuggingFace Spaces. An example of a space we've created for deploying Windanam can be found here: https://huggingface.co/spaces/cawoylel/MMS-ASR-Fula. However, this can be quite slow as the model will be hosted on CPUs rather than GPUs.\n",
        "Alternatively, you can deploy your model using traditional compute providers like Google Cloud Platform, AWS, or Azure.\n",
        "It all depends on the number of people who will use your model and the budget you have."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
