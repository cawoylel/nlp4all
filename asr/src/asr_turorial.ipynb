{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqJnEAUgjdpaYFcMdO+9a7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yaya-sy/nlp4all/blob/main/asr/src/asr_turorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "-jKa71Y6_VYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "4ZM5ZQKY_SEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install libsox-fmt-all sox ffmpeg # needed for processing audio\n",
        "!apt install libicu-dev pkg-config # needed for processing text and unicode symbols"
      ],
      "metadata": {
        "id": "ddzlqgyTNrVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchaudio torchvision -y # we need to install the nightly version of torch\n",
        "!pip install -q --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
        "!pip install -q fairseq # we will use this package for aligning the audio-text\n",
        "!pip install -q dataclasses\n",
        "!pip install -q sox # for audio processing\n",
        "!pip install -q scrapy # for scapping the data\n",
        "!pip install -q ICU-Tokenizer # for tokenizing the text\n",
        "!pip install -q transformers # we will use huggingface transformers for training the models\n",
        "!pip install -q datasets # we will use huggingface datasets for loading the training dataset\n",
        "!pip install -q librosa # required by huggingface dataset\n",
        "!pip install -q evaluate # for evaluating the models\n",
        "!pip install -q jiwer # for computing WER (Word Error Rate)\n",
        "!pip install -q bitsandbytes # for loading the quantized model\n",
        "!pip install -q accelerate # for efficient training and evaluation\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main # for lightweight training using LoRA"
      ],
      "metadata": {
        "id": "8O-nLC2sN3UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cawoylel/nlp4all.git # repository containing the code of this tutorial\n",
        "!git clone https://github.com/isi-nlp/uroman.git"
      ],
      "metadata": {
        "id": "Ty-GLgwQ_VOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "git clone https://github.com/facebookresearch/fairseq.git\n",
        "cd fairseq\n",
        "pip install --editable ./"
      ],
      "metadata": {
        "id": "gYVw62egXVtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"openai/whisper-large-v3\"\n",
        "task = \"transcribe\"\n",
        "language = \"Hausa\"\n",
        "language_abbr = \"ha\""
      ],
      "metadata": {
        "id": "kLLeYERBH80s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the ASR data"
      ],
      "metadata": {
        "id": "5hkzQTtLLT2v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bible Crawler"
      ],
      "metadata": {
        "id": "0Uv-NfY6Ljre"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5stE19ySKhO4"
      },
      "outputs": [],
      "source": [
        "from scrapy.crawler import CrawlerProcess\n",
        "from icu_tokenizer import SentSplitter\n",
        "from nlp4all.asr.src.scraper import BibleScraper"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SPLITTER = SentSplitter()"
      ],
      "metadata": {
        "id": "Q8psgKPNL8gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process = CrawlerProcess()\n",
        "process.crawl(BibleScraper,\n",
        "              name=\"SeereerBible\",\n",
        "              output_folder=\"SeereerBible\",\n",
        "              start_urls=[\"https://www.bible.com/bible/3751/GEN.1.SRR23\"], # You can change this according\n",
        "              language=\"Sereer-Sine\",\n",
        "              code=\"SRR23\",\n",
        "              splitter=SentSplitter())"
      ],
      "metadata": {
        "id": "CJuuwbiaMEuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process.start()"
      ],
      "metadata": {
        "id": "Ze7dOj6Ose5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resampling the audios"
      ],
      "metadata": {
        "id": "MXa7lmZ-CV0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After downloading the audios, we need to resample the audios. Many modern speech models only deal with 16k sampling. We will use ffmpeg to resample the audios into 16k and also to convert them to .wav file."
      ],
      "metadata": {
        "id": "GyZtr8YXCaBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "for f in /content/SeereerBible/raw/Sereer-Sine/*.mp3; do\n",
        "  filename=\"$(basename \"$f\")\"\n",
        "  directory=\"$(dirname \"$f\")\"\n",
        "  stem=${filename%.*}\n",
        "  ffmpeg -i $f -ac 1 -ar 16000 $directory/$stem.wav ;\n",
        "done"
      ],
      "metadata": {
        "id": "BQOR_M2AQqDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Forced Alignment"
      ],
      "metadata": {
        "id": "mHPQxYv3M9oq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As such, we cannot use the data in order to train an ASR model. Because the audios are too long. We ideally need segment less than 30s long and the corresponding text.\n",
        "\n",
        "Splitting the audios into short segments while finding the corresponding transcription is what called \"forced alignment\"."
      ],
      "metadata": {
        "id": "ZHqkn2_OCz4Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "input_folder=/content/SeereerBible/raw/Sereer-Sine\n",
        "output_folder=/content/SeereerBible/aligned/Sereer-Sine\n",
        "cd fairseq/\n",
        "for text in $input_folder/*.txt; do\n",
        "  filename=\"$(basename \"$text\")\"\n",
        "  stem=${filename%.*}\n",
        "  output_path=$output_folder/$stem\n",
        "  rm -rf $output_path\n",
        "  python -m examples.mms.data_prep.align_and_segment \\\n",
        "  --audio_filepath $input_folder/$stem.wav \\\n",
        "  --text_filepath $input_folder/$stem.txt \\\n",
        "  --lang ful \\\n",
        "  --outdir $output_path \\\n",
        "  --uroman /content/uroman/bin\n",
        "done"
      ],
      "metadata": {
        "id": "BOGlte_SOe8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the HuggingFace dataset"
      ],
      "metadata": {
        "id": "g6RMLHQ6Ilie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.disable(logging.DEBUG)\n",
        "logging.disable(logging.INFO)\n",
        "logging.disable(logging.WARNING)"
      ],
      "metadata": {
        "id": "qYqmxNZalBwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import datasets\n",
        "# import logging\n",
        "# numba_logger = logging.getLogger('numba')\n",
        "# numba_logger.setLevel(logging.WARNING)"
      ],
      "metadata": {
        "id": "HAwDGUKQJmac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = datasets.Features(\n",
        "    {\n",
        "        \"audio\": datasets.features.Audio(sampling_rate=16_000),\n",
        "        \"transcription\": datasets.Value(\"string\"),\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "id": "AOyRZtxKU4c_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def manifest_to_folder(input_folder: str):\n",
        "    input_folder = Path(input_folder)\n",
        "    for chapter in input_folder.glob(\"*/\"):\n",
        "        with open(chapter / \"manifest.json\", \"r\") as manifest:\n",
        "            for line in manifest:\n",
        "                data = eval(line)\n",
        "                audio_filepath = data[\"audio_filepath\"]\n",
        "                text = data[\"text\"]\n",
        "                yield {\n",
        "                    \"audio\": audio_filepath,\n",
        "                    \"transcription\": text\n",
        "                }"
      ],
      "metadata": {
        "id": "xEJPvsbb7-cF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = datasets.Dataset.from_generator(manifest_to_folder,\n",
        "                                          features=features,\n",
        "                                          gen_kwargs={\"input_folder\": \"/content/SeereerBible/aligned/Sereer-Sine\"}\n",
        "                                          ).cast_column(\"audio\", datasets.Audio())"
      ],
      "metadata": {
        "id": "e3_z7sSzKdnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the dataset"
      ],
      "metadata": {
        "id": "xby4myb3Hv6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import WhisperFeatureExtractor\n",
        "from transformers import WhisperTokenizer\n",
        "from transformers import WhisperProcessor"
      ],
      "metadata": {
        "id": "qqOo5RQaHyK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
        "processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)"
      ],
      "metadata": {
        "id": "-B7IwNKFIAf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"transcription\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "7pQCCFRmKjuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_proc = os.cpu_count()\n",
        "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names, num_proc=num_proc)\n",
        "dataset = dataset.train_test_split(0.1)"
      ],
      "metadata": {
        "id": "g9CBdTQOKrCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "hHsygYJiaWNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune Whisper Large on Seereer ASR dataset"
      ],
      "metadata": {
        "id": "ghpi9gDjB28x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration, Seq2SeqTrainer, TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
        "from transformers import Seq2SeqTrainingArguments\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, PeftModel, LoraModel, LoraConfig, get_peft_model\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union"
      ],
      "metadata": {
        "id": "C7fpfTjGMlNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the model"
      ],
      "metadata": {
        "id": "BsxblQujMzuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = WhisperForConditionalGeneration.from_pretrained(model_name_or_path, load_in_8bit=True, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "uvizn71iMvkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the pretrained model for LoRA"
      ],
      "metadata": {
        "id": "lMWTE0jaNx7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "model.model.encoder.conv1.register_forward_hook(lambda module, input, output: output.requires_grad_(True))"
      ],
      "metadata": {
        "id": "siySu0eQM_cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attach adapters to the pretrained model"
      ],
      "metadata": {
        "id": "X8nLlYxxN0sX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = LoraConfig(r=32, lora_alpha=64, target_modules=[\"q_proj\", \"v_proj\"], lora_dropout=0.05, bias=\"none\")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "BU7Opfu2N_jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This callback helps to save only the adapter weights and remove the base model weights.\n",
        "class SavePeftModelCallback(TrainerCallback):\n",
        "    def on_save(\n",
        "        self,\n",
        "        args: TrainingArguments,\n",
        "        state: TrainerState,\n",
        "        control: TrainerControl,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        checkpoint_folder = os.path.join(args.output_dir, f\"{PREFIX_CHECKPOINT_DIR}-{state.global_step}\")\n",
        "\n",
        "        peft_model_path = os.path.join(checkpoint_folder, \"adapter_model\")\n",
        "        kwargs[\"model\"].save_pretrained(peft_model_path)\n",
        "\n",
        "        pytorch_model_path = os.path.join(checkpoint_folder, \"pytorch_model.bin\")\n",
        "        if os.path.exists(pytorch_model_path):\n",
        "            os.remove(pytorch_model_path)\n",
        "        return control"
      ],
      "metadata": {
        "id": "UaPV7-AmPoTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataloader"
      ],
      "metadata": {
        "id": "74MzKS0HPXoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "rdKKAckZMSs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "3IlRuDIsMVAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training arguments"
      ],
      "metadata": {
        "id": "vAiiH2WRPdnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"reach-vb/test\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-3,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    per_device_eval_batch_size=8,\n",
        "    generation_max_length=128,\n",
        "    logging_steps=100,\n",
        "    max_steps=100, # only for testing purposes, remove this from your final run :)\n",
        "    remove_unused_columns=False,  # required as the PeftModel forward doesn't have the signature of the wrapped model's forward\n",
        "    label_names=[\"labels\"],  # same reason as above\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "ECWQpHN0Pfaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the training"
      ],
      "metadata": {
        "id": "zM4xi3BZQzAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "y6Xh_Rc0XBH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        "    callbacks=[SavePeftModelCallback],\n",
        ")\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
      ],
      "metadata": {
        "id": "7cZAE8VgQ1JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZovwoAU5Q3zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "PKFw3-rIyJMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "eval_dataloader = DataLoader(dataset[\"test\"], batch_size=8, collate_fn=data_collator)\n",
        "forced_decoder_ids = processor.get_decoder_prompt_ids(language=language, task=task)\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "predictions = []\n",
        "references = []\n",
        "normalized_predictions = []\n",
        "normalized_references = []\n",
        "\n",
        "model.eval()\n",
        "for step, batch in enumerate(tqdm(eval_dataloader)):\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = (\n",
        "                model.generate(\n",
        "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
        "                    forced_decoder_ids=forced_decoder_ids,\n",
        "                    max_new_tokens=32,\n",
        "                )\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "            labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
        "            decoded_preds = processor.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "            decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            predictions.extend(decoded_preds)\n",
        "            references.extend(decoded_labels)\n",
        "            normalized_predictions.extend([normalizer(pred).strip() for pred in decoded_preds])\n",
        "            normalized_references.extend([normalizer(label).strip() for label in decoded_labels])\n",
        "        del generated_tokens, labels, batch\n",
        "    gc.collect()\n",
        "wer = 100 * metric.compute(predictions=predictions, references=references)\n",
        "normalized_wer = 100 * metric.compute(predictions=normalized_predictions, references=normalized_references)\n",
        "eval_metrics = {\"eval/wer\": wer, \"eval/normalized_wer\": normalized_wer}\n",
        "\n",
        "print(f\"{wer=} and {normalized_wer=}\")\n",
        "print(eval_metrics)"
      ],
      "metadata": {
        "id": "34jpYPFjgnlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(predictions, references))"
      ],
      "metadata": {
        "id": "OiieDIAZyPL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-jCBsbWyxSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}